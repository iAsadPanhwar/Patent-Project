{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OKVYQxFYyXb"
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "gcbgTss4b_Iy",
    "outputId": "65901905-eba5-4726-a862-aa55e7cb6a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crewai==0.76.9 in /usr/local/lib/python3.10/dist-packages (0.76.9)\n",
      "Requirement already satisfied: crewai_tools==0.13.4 in /usr/local/lib/python3.10/dist-packages (0.13.4)\n",
      "Requirement already satisfied: langchain_community==0.3.5 in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (1.4.4)\n",
      "Requirement already satisfied: auth0-python>=4.7.1 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (4.7.2)\n",
      "Requirement already satisfied: chromadb>=0.4.24 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (0.5.18)\n",
      "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (8.1.7)\n",
      "Requirement already satisfied: instructor>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (1.6.4)\n",
      "Requirement already satisfied: json-repair>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (0.30.2)\n",
      "Requirement already satisfied: jsonref>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (1.1.0)\n",
      "Requirement already satisfied: langchain>=0.2.16 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (0.3.7)\n",
      "Requirement already satisfied: litellm>=1.44.22 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (1.52.9)\n",
      "Requirement already satisfied: openai>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (1.54.4)\n",
      "Requirement already satisfied: opentelemetry-api>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (1.28.2)\n",
      "Requirement already satisfied: pydantic>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (2.9.2)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (1.0.1)\n",
      "Requirement already satisfied: pyvis>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (0.3.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (2024.9.11)\n",
      "Requirement already satisfied: tomli-w>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (1.1.0)\n",
      "Requirement already satisfied: tomli>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (2.1.0)\n",
      "Requirement already satisfied: uv>=0.4.25 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (0.5.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.13.4) (4.12.3)\n",
      "Requirement already satisfied: docker>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.13.4) (7.1.0)\n",
      "Requirement already satisfied: docx2txt>=0.8 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.13.4) (0.8)\n",
      "Requirement already satisfied: embedchain>=0.1.114 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.13.4) (0.1.125)\n",
      "Requirement already satisfied: lancedb>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.13.4) (0.16.0)\n",
      "Requirement already satisfied: pyright>=1.1.350 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.13.4) (1.1.389)\n",
      "Requirement already satisfied: pytest>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.13.4) (8.3.3)\n",
      "Requirement already satisfied: pytube>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.13.4) (15.0.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.13.4) (2.32.3)\n",
      "Requirement already satisfied: selenium>=4.18.1 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.13.4) (4.26.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (3.11.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (0.4.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (0.3.18)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (0.1.143)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (2.6.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (4.0.3)\n",
      "Requirement already satisfied: cryptography<44.0.0,>=43.0.1 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai==0.76.9) (43.0.3)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai==0.76.9) (2.9.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=2.0.7 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai==0.76.9) (2.2.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.12.3->crewai_tools==0.13.4) (2.6)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (0.115.5)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai==0.76.9) (0.32.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (0.49b2)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (0.20.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (4.66.6)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (1.68.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (0.13.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (31.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (3.10.11)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (13.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.5) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.5) (0.9.0)\n",
      "Requirement already satisfied: alembic<2.0.0,>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai_tools==0.13.4) (1.14.0)\n",
      "Requirement already satisfied: cohere<6.0,>=5.3 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai_tools==0.13.4) (5.11.4)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.26.1 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai_tools==0.13.4) (1.71.1)\n",
      "Requirement already satisfied: gptcache<0.2.0,>=0.1.43 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai_tools==0.13.4) (0.1.44)\n",
      "Requirement already satisfied: langchain-cohere<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai_tools==0.13.4) (0.3.1)\n",
      "Requirement already satisfied: langchain-openai<0.3.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai_tools==0.13.4) (0.2.8)\n",
      "Requirement already satisfied: mem0ai<0.2.0,>=0.1.29 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai_tools==0.13.4) (0.1.29)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai_tools==0.13.4) (5.1.0)\n",
      "Requirement already satisfied: pysbd<0.4.0,>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai_tools==0.13.4) (0.3.4)\n",
      "Requirement already satisfied: schema<0.8.0,>=0.7.5 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai_tools==0.13.4) (0.7.7)\n",
      "Requirement already satisfied: tiktoken<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai_tools==0.13.4) (0.7.0)\n",
      "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor>=1.3.3->crewai==0.76.9) (0.16)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from instructor>=1.3.3->crewai==0.76.9) (3.1.4)\n",
      "Requirement already satisfied: jiter<0.7,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from instructor>=1.3.3->crewai==0.76.9) (0.6.1)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor>=1.3.3->crewai==0.76.9) (2.23.4)\n",
      "Requirement already satisfied: deprecation in /usr/local/lib/python3.10/dist-packages (from lancedb>=0.5.4->crewai_tools==0.13.4) (2.1.0)\n",
      "Requirement already satisfied: nest-asyncio~=1.0 in /usr/local/lib/python3.10/dist-packages (from lancedb>=0.5.4->crewai_tools==0.13.4) (1.6.0)\n",
      "Requirement already satisfied: pylance==0.19.2 in /usr/local/lib/python3.10/dist-packages (from lancedb>=0.5.4->crewai_tools==0.13.4) (0.19.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lancedb>=0.5.4->crewai_tools==0.13.4) (24.2)\n",
      "Requirement already satisfied: pyarrow>=12 in /usr/local/lib/python3.10/dist-packages (from pylance==0.19.2->lancedb>=0.5.4->crewai_tools==0.13.4) (17.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.2.16->crewai==0.76.9) (0.3.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_community==0.3.5) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community==0.3.5) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.22->crewai==0.76.9) (8.5.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.22->crewai==0.76.9) (4.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.13.3->crewai==0.76.9) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.13.3->crewai==0.76.9) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.13.3->crewai==0.76.9) (1.3.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.22.0->crewai==0.76.9) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai==0.76.9) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai==0.76.9) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai==0.76.9) (1.28.2)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-proto==1.28.2->opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai==0.76.9) (5.28.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk>=1.22.0->crewai==0.76.9) (0.49b2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4.2->crewai==0.76.9) (0.7.0)\n",
      "Requirement already satisfied: nodeenv>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from pyright>=1.1.350->crewai_tools==0.13.4) (1.9.1)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=8.0.0->crewai_tools==0.13.4) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest>=8.0.0->crewai_tools==0.13.4) (1.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=8.0.0->crewai_tools==0.13.4) (1.2.2)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis>=0.3.2->crewai==0.76.9) (7.34.0)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis>=0.3.2->crewai==0.76.9) (4.0.0)\n",
      "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis>=0.3.2->crewai==0.76.9) (3.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->crewai_tools==0.13.4) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->crewai_tools==0.13.4) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->crewai_tools==0.13.4) (2024.8.30)\n",
      "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium>=4.18.1->crewai_tools==0.13.4) (0.27.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium>=4.18.1->crewai_tools==0.13.4) (0.11.1)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium>=4.18.1->crewai_tools==0.13.4) (1.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain_community==0.3.5) (3.1.1)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic<2.0.0,>=1.13.1->embedchain>=0.1.114->crewai_tools==0.13.4) (1.3.6)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb>=0.4.24->crewai==0.76.9) (1.2.0)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai_tools==0.13.4) (1.9.7)\n",
      "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai_tools==0.13.4) (0.9.0)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai_tools==0.13.4) (2.32.0.20241016)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<44.0.0,>=43.0.1->auth0-python>=4.7.1->crewai==0.76.9) (1.17.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.22.0->crewai==0.76.9) (1.16.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb>=0.4.24->crewai==0.76.9) (0.41.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (2.19.2)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (2.27.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (1.25.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (2.8.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (1.13.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (2.0.6)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from gptcache<0.2.0,>=0.1.43->embedchain>=0.1.114->crewai_tools==0.13.4) (5.5.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb>=0.4.24->crewai==0.76.9) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=0.4.24->crewai==0.76.9) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.44.22->crewai==0.76.9) (3.21.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (75.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (0.19.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (5.7.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (3.0.48)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (2.18.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor>=1.3.3->crewai==0.76.9) (3.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_community==0.3.5) (3.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai==0.76.9) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai==0.76.9) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai==0.76.9) (0.21.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai==0.76.9) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai==0.76.9) (2.8.2)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai==0.76.9) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai==0.76.9) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai==0.76.9) (0.9)\n",
      "Requirement already satisfied: langchain-experimental>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools==0.13.4) (0.3.3)\n",
      "Requirement already satisfied: pandas>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools==0.13.4) (2.2.2)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools==0.13.4) (0.9.0)\n",
      "Requirement already satisfied: pytz<2025.0,>=2024.1 in /usr/local/lib/python3.10/dist-packages (from mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4) (2024.2)\n",
      "Requirement already satisfied: qdrant-client<2.0.0,>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4) (1.12.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.24->crewai==0.76.9) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.24->crewai==0.76.9) (24.3.25)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.24->crewai==0.76.9) (1.13.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.24->crewai==0.76.9) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.24->crewai==0.76.9) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.24->crewai==0.76.9) (0.49b2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.24->crewai==0.76.9) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb>=0.4.24->crewai==0.76.9) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb>=0.4.24->crewai==0.76.9) (2.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb>=0.4.24->crewai==0.76.9) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb>=0.4.24->crewai==0.76.9) (0.26.2)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium>=4.18.1->crewai_tools==0.13.4) (2.4.0)\n",
      "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium>=4.18.1->crewai_tools==0.13.4) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium>=4.18.1->crewai_tools==0.13.4) (1.2.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb>=0.4.24->crewai==0.76.9) (1.5.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.5) (1.0.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium>=4.18.1->crewai_tools==0.13.4) (1.7.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai==0.76.9) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai==0.76.9) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai==0.76.9) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai==0.76.9) (14.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<44.0.0,>=43.0.1->auth0-python>=4.7.1->crewai==0.76.9) (2.22)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (1.62.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (2.7.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (0.13.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=0.4.24->crewai==0.76.9) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=0.4.24->crewai==0.76.9) (2024.10.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.4.24->crewai==0.76.9) (0.1.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools==0.13.4) (2024.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (0.2.13)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4) (1.68.0)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4) (2.10.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.4.24->crewai==0.76.9) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.4.24->crewai==0.76.9) (1.3.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (1.6.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4) (4.1.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (0.6.1)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install crewai==0.76.9 crewai_tools==0.13.4 langchain_community==0.3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "iP3Dq4cOBlpw"
   },
   "outputs": [],
   "source": [
    "#!pip install CrewAI crewai_tools langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "Y-XsXo9YCa-C"
   },
   "outputs": [],
   "source": [
    "#!pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltjVe7aCH_VR",
    "outputId": "18c0130a-b122-4645-ae19-e33ed60397e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain-groq) (0.12.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-groq) (0.3.18)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-groq) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-groq) (0.1.143)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-groq) (24.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-groq) (9.0.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-groq) (3.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-groq) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-groq) (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EwZrSdPed_6M"
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5buKQOIT9ZyO"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from crewai import Agent, Task, Crew, Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "tGt6JOL1UdGr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "# os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
    "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API_KEY')\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"placeholder_key\"\n",
    "\n",
    "\n",
    "\n",
    "from google.colab import userdata\n",
    "openai_api_key = userdata.get('OPEN_AI_API')\n",
    "serper_api_key = userdata.get('SERPER_API_KEY')\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPEN_AI_API')\n",
    "os.environ[\"OPENAI_MODEL_NAME\"] = 'gpt-4o-mini'\n",
    "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ob_a718LsEBh"
   },
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "# from crewai import LLM\n",
    "\n",
    "# llm = LLM(\n",
    "#     model=\"groq/llama-3.1-8b-instant\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6Nyseb6TcQF"
   },
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "JJwAitztR0zp",
    "outputId": "875c4e63-331d-4632-b4d8-d06a73c2daa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# # Creating tools\n",
    "!pip install requests  # Install the requests library\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sW1LyCBzn2yi"
   },
   "outputs": [],
   "source": [
    "# Patent search tool NEW.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from crewai_tools import BaseTool\n",
    "\n",
    "class PatentSearchTool(BaseTool):\n",
    "    name: str = \"Patent Search tool\"\n",
    "    description: str = \"Search the internet for Patents.\"\n",
    "\n",
    "    # @record_tool(tool_name=\"Patent search tool\")\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Search the internet for Patents.\n",
    "        \"\"\"\n",
    "\n",
    "        url = \"https://google.serper.dev/patents\"\n",
    "\n",
    "        payload = json.dumps({\n",
    "            \"q\": query,\n",
    "            \"num\": 5,\n",
    "            \"autocorrect\": False,\n",
    "            \"tbs\": \"qdr:d\"\n",
    "        })\n",
    "\n",
    "        headers = {\n",
    "            'X-API-KEY': os.getenv('SERPER_API_KEY'),\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON response\n",
    "            response_data = response.json()\n",
    "\n",
    "            # Log the entire response structure for debugging\n",
    "            print(\"Response data structure:\", json.dumps(response_data, indent=2))\n",
    "\n",
    "            # Ensure 'figures' exists and is a list, and limit to 1 figure\n",
    "            if 'figures' in response_data and isinstance(response_data['figures'], list):\n",
    "                if response_data['figures']:  # Check if the list is not empty\n",
    "                    response_data['figures'] = [response_data['figures'][0]]  # Keep only the first figure\n",
    "                else:\n",
    "                    response_data['figures'] = []  # If no figures, set to an empty list\n",
    "            else:\n",
    "                response_data['figures'] = []  # If 'figures' key does not exist, set to an empty list\n",
    "\n",
    "            # Ensure only one URL and thumbnail, assuming they are nested under 'results' (adjust based on actual API response)\n",
    "            if 'results' in response_data and isinstance(response_data['results'], list) and response_data['results']:\n",
    "                first_result = response_data['results'][0]  # Get the first result only\n",
    "                response_data['results'] = [first_result]  # Replace results with only the first result\n",
    "\n",
    "                # Optionally, if URLs and thumbnails are within each result\n",
    "                if 'url' in first_result:\n",
    "                    first_result['url'] = first_result['url']\n",
    "                if 'thumbnail' in first_result:\n",
    "                    first_result['thumbnail'] = first_result['thumbnail']\n",
    "\n",
    "            # Convert the response data to a JSON string and return\n",
    "            return json.dumps(response_data, indent=2)\n",
    "\n",
    "        # Handle non-200 status code\n",
    "        return f\"Error: Received status code {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pYMHMzjD4oug"
   },
   "outputs": [],
   "source": [
    "# search scholar\n",
    "\n",
    "class ScholarSearchTool(BaseTool):\n",
    "    name: str = \"Scholar search tool\"\n",
    "    description: str = \"Search the internet for academic articles.\"\n",
    "\n",
    "    # @record_tool(tool_name=\"Scholar Search Tool\")\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Search the internet for academic articles.\n",
    "        \"\"\"\n",
    "\n",
    "        url = \"https://google.serper.dev/scholar\"\n",
    "\n",
    "        payload = json.dumps({\n",
    "            \"q\": query,\n",
    "            \"num\": 5,\n",
    "            \"autocorrect\": False,\n",
    "            \"tbs\": \"qdr:d\"\n",
    "        })\n",
    "\n",
    "        headers = {\n",
    "            'X-API-KEY': os.getenv('SERPER_API_KEY'),\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON response\n",
    "            response_data = response.json()\n",
    "\n",
    "        # Convert the news data back to a JSON string\n",
    "        return json.dumps(response_data, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Z3xwhuvy0m8y"
   },
   "outputs": [],
   "source": [
    "# search news\n",
    "\n",
    "# class NewsSearchTool(BaseTool):\n",
    "#     name: str = \"News search tool\"\n",
    "#     description: str = \"Search the internet for news on the given topic.\"\n",
    "\n",
    "#     # @record_tool(tool_name=\"Scholar Search Tool\")\n",
    "#     def _run(self, query: str) -> str:\n",
    "#         \"\"\"\n",
    "#         Search the internet for news on the given topic.\n",
    "#         \"\"\"\n",
    "\n",
    "#         url = \"https://google.serper.dev/news\"\n",
    "\n",
    "#         payload = json.dumps({\n",
    "#             \"q\": query,\n",
    "#             \"num\": 5,\n",
    "#             \"autocorrect\": False,\n",
    "#             \"tbs\": \"qdr:d\"\n",
    "#         })\n",
    "\n",
    "#         headers = {\n",
    "#             'X-API-KEY': os.getenv('SERPER_API_KEY'),\n",
    "#             'Content-Type': 'application/json'\n",
    "#         }\n",
    "\n",
    "#         response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "#         # Check if the request was successful (status code 200)\n",
    "#         if response.status_code == 200:\n",
    "#             # Parse the JSON response\n",
    "#             response_data = response.json()\n",
    "\n",
    "#         # Convert the news data back to a JSON string\n",
    "#         return json.dumps(response_data, indent=2)\n",
    "\n",
    "\n",
    "class NewsSearchTool(BaseTool):\n",
    "    name: str = \"Custom Serper Dev Tool\"\n",
    "    description: str = \"Search the internet for news.\"\n",
    "\n",
    "    # @record_tool(tool_name=\"Custom Serper Dev Tool\")\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Search the internet for news.\n",
    "        \"\"\"\n",
    "\n",
    "        url = \"https://google.serper.dev/news\"\n",
    "\n",
    "        payload = json.dumps({\n",
    "            \"q\": query,\n",
    "            \"num\": 10,\n",
    "            \"autocorrect\": False,\n",
    "            \"tbs\": \"qdr:d\"\n",
    "        })\n",
    "\n",
    "        headers = {\n",
    "            'X-API-KEY': os.getenv('SERPER_API_KEY'),\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON response\n",
    "            response_data = response.json()\n",
    "\n",
    "            # Log the entire response structure for debugging\n",
    "            print(\"Response data structure:\", json.dumps(response_data, indent=2))\n",
    "\n",
    "            # Ensure 'news' exists and is a list\n",
    "            if 'news' in response_data and isinstance(response_data['news'], list):\n",
    "                if response_data['news']:  # Check if the list is not empty\n",
    "                    response_data['news'] = [response_data['news'][0]]  # Keep only the first news item\n",
    "                else:\n",
    "                    response_data['news'] = []  # If no news items, set to an empty list\n",
    "            else:\n",
    "                response_data['news'] = []  # If 'news' key does not exist, set to an empty list\n",
    "\n",
    "            # Convert the response data to a JSON string\n",
    "            return json.dumps(response_data, indent=2)\n",
    "\n",
    "        # Handle non-200 status code\n",
    "        return f\"Error: Received status code {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ENQuKY7pqQZf"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Custom file writer function\n",
    "def write_to_file(content, directory='project', filename='output.txt'):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    # Ensure content is a string before writing\n",
    "    if not isinstance(content, str):\n",
    "        content = str(content)\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "    print(f\"Content written to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SfWdHDNfThtn"
   },
   "outputs": [],
   "source": [
    "from crewai_tools import DirectoryReadTool, \\\n",
    "                         FileReadTool, \\\n",
    "                         SerperDevTool, \\\n",
    "                         DallETool, \\\n",
    "                         ScrapeWebsiteTool, \\\n",
    "                         WebsiteSearchTool, \\\n",
    "                         VisionTool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nQJ07OVzf31h"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Z9Ofk5xJ0pO_"
   },
   "outputs": [],
   "source": [
    "#Access Google Drive to write files\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gYXyYqRRT7i4"
   },
   "outputs": [],
   "source": [
    "#Reading and writing\n",
    "\n",
    "#read_inputs = DirectoryReadTool(directory='/content/drive/MyDrive/Creative_AI/research_engine/Inputs')\n",
    "#read_outputs = DirectoryReadTool(directory='/content/drive/MyDrive/Creative_AI/research_engine/Outputs')\n",
    "#read_baseline= FileReadTool(file_path='/content/drive/MyDrive/Creative_AI/research_engine/Outputs/knowledge_baseline.txt')\n",
    "#file_read_tool = FileReadTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "id": "vxp0wYEHzij6"
   },
   "outputs": [],
   "source": [
    "# Direct file reading using Python standard library\n",
    "#file_path = '/content/drive/MyDrive/Creative_AI/research_engine/Inputs/test.txt'\n",
    "#with open(file_path, 'r') as file:\n",
    "#    content = file.read()\n",
    "\n",
    "#Check if content is being read\n",
    "#print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jXOiO9Uc1aw0"
   },
   "outputs": [],
   "source": [
    "#instantiating other tools\n",
    "\n",
    "search_tool = SerperDevTool()\n",
    "scrape_tool = ScrapeWebsiteTool()\n",
    "website_search_tool = WebsiteSearchTool()\n",
    "\n",
    "#vision_tool = VisionTool(model=\"dall-e-3\",\n",
    "#                       size=\"1024x1024\",\n",
    "#                       quality=\"standard\",\n",
    "#                       n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvXe63584xPr"
   },
   "source": [
    "# AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VTLs_sK_cSh-"
   },
   "outputs": [],
   "source": [
    "manager = Agent(\n",
    "    role=\"Project Manager\",\n",
    "    goal=\"Efficiently manage the research team and ensure the production of world-class research reports\",\n",
    "    backstory=(\n",
    "        \"You are a highly experienced research project manager, overseeing a team dedicated to producing exceptional research reports. \"\n",
    "        \"Your responsibilities include ensuring quality at each stage: knowledge collection, analysis, interpretation, and reporting. \"\n",
    "        \"Implement multiple revision loops, verifying each stage for accuracy, completeness, and relevance to the topic, purpose, and context.\"\n",
    "        \"\\n\\nInstructions:\\n\"\n",
    "        \"1. **Knowledge Collection**: Begin by collecting all relevant knowledge on the {topic} within the {context}. Gather only reliable and relevant sources.\\n\"\n",
    "        \"2. **Analysis & Interpretation**: Conduct a detailed analysis, identifying key trends, insights, and implications. Ensure interpretations are accurate and based on the collected data.\\n\"\n",
    "        \"3. **Quality Checks**: After each stage, evaluate completeness, accuracy, and relevance to {purpose}. If content is lacking, instruct the team to revise it accordingly.\\n\"\n",
    "        \"4. **Reporting**: Guide the team in drafting a final report. Structure it clearly with an introduction, analysis, and conclusion sections.\\n\"\n",
    "        \"If any stage is incomplete or fails to meet standards, delegate tasks back to team members for further improvement.\\n\"\n",
    "        \"Research topic: {topic}\\n\"\n",
    "        \"Research purpose: {purpose}\\n\"\n",
    "        \"Research context: {context}\\n\"\n",
    "    ),\n",
    "    # llm=llm,\n",
    "    allow_delegation=True,\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    # max_iter=2,  # Uncomment to limit iterations for efficiency\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_HzMl_4-XbdA"
   },
   "outputs": [],
   "source": [
    "strategist_agent = Agent(\n",
    "    role=\"Strategist\",\n",
    "    goal=\"Analyze and unpack the challenge to reveal its core components and actionable steps\",\n",
    "    backstory=(\n",
    "        \"As an expert strategist, you excel at breaking down complex challenges into clear, manageable elements. \"\n",
    "        \"Your goal is to analyze the challenge thoroughly, identifying core issues, key factors, and any underlying assumptions. \"\n",
    "        \"From this analysis, outline a clear strategy, highlighting priority areas and actionable steps to address the challenge effectively.\"\n",
    "    ),\n",
    "    instructions=(\n",
    "        \"1. **Identify Core Elements**: Begin by identifying and listing the main components of the challenge. \"\n",
    "        \"Break down complex issues into their constituent parts to make them easier to understand.\\n\"\n",
    "        \"2. **Analyze Key Factors**: Examine each core element, identifying any factors that impact the challenge, such as constraints, risks, and opportunities.\\n\"\n",
    "        \"3. **Outline Assumptions**: Highlight any assumptions or uncertainties that might influence the approach to solving the challenge.\\n\"\n",
    "        \"4. **Recommend Actionable Steps**: Conclude by recommending actionable steps based on the analysis, prioritizing actions that are likely to have the highest impact.\\n\"\n",
    "    ),\n",
    "    # llm=llm,\n",
    "    allow_delegation=False,\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    # max_iter=2  # Uncomment to limit the iterations for efficiency\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Tmxwqm-GYWaS"
   },
   "outputs": [],
   "source": [
    "researcher_agent = Agent(\n",
    "    role=\"Researcher\",\n",
    "    goal=\"Locate and summarize relevant information to answer questions accurately and comprehensively\",\n",
    "    backstory=(\n",
    "        \"As a diligent and methodical researcher, you are tasked with gathering accessible and highly relevant information \"\n",
    "        \"using the tools available. You excel at sifting through large amounts of information, extracting key points, \"\n",
    "        \"and presenting them concisely.\"\n",
    "        \"\\n\\nInstructions:\\n\"\n",
    "        \"1. **Identify Relevant Sources**: Focus on finding credible and authoritative sources that address the question directly.\\n\"\n",
    "        \"2. **Extract Key Points**: From each source, extract only the most relevant details. Avoid overly general information or unrelated data.\\n\"\n",
    "        \"3. **Summarize Effectively**: Summarize findings in a clear, concise manner. If there are multiple aspects to the question, organize responses by topic.\\n\"\n",
    "        \"4. **Verify for Completeness**: Ensure that all aspects of the question are addressed comprehensively. If a gap remains, indicate further research areas.\\n\"\n",
    "    ),\n",
    "    # llm=llm,\n",
    "    allow_delegation=False,\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    # max_iter=4  # Uncomment to limit iterations for efficiency\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "KOYgcTFVtZ83"
   },
   "outputs": [],
   "source": [
    "scraper_agent = Agent(\n",
    "    role=\"Scraper\",\n",
    "    goal=\"Extract and structure key details from patent documents efficiently\",\n",
    "    backstory=(\n",
    "        \"As an expert scraper, you are skilled at extracting and organizing detailed information from patent documents. \"\n",
    "        \"Your role is to retrieve abstracts, claims, technical descriptions, and other relevant information with precision, \"\n",
    "        \"ensuring accuracy for subsequent analysis and reporting.\"\n",
    "    ),\n",
    "    instructions=(\n",
    "        \"1. **Identify Key Sections**: Locate and extract the patent's abstract, claims, and technical descriptions. \"\n",
    "        \"Pay particular attention to details relevant to the invention's novelty, application, and technical aspects.\\n\"\n",
    "        \"2. **Structure Data**: Organize extracted information into a structured format, separating sections such as abstract, claims, inventor information, and classifications.\\n\"\n",
    "        \"3. **Check for Completeness**: Verify that each key section (abstract, claims, and technical descriptions) is captured accurately and is free from omissions.\\n\"\n",
    "        \"4. **Flag Issues**: If any section is missing or lacks clarity, indicate these issues clearly for further review.\\n\"\n",
    "    ),\n",
    "    # llm=llm,\n",
    "    allow_delegation=False,\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    # max_iter=2  # Uncomment to limit iterations for efficiency\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GkFE1HR2taqJ"
   },
   "outputs": [],
   "source": [
    "writer_agent = Agent(\n",
    "    role=\"Writer\",\n",
    "    goal=\"Summarize technical information from patents into clear, accessible formats for a broad audience\",\n",
    "    backstory=(\n",
    "        \"As a skilled technical writer, you excel at translating complex technical information from patents into concise and \"\n",
    "        \"understandable summaries. You focus on key innovations, core claims, and potential applications, ensuring that both technical \"\n",
    "        \"and non-technical audiences can grasp the main points effectively.\"\n",
    "    ),\n",
    "    instructions=(\n",
    "        \"1. **Identify Key Innovations**: Begin by summarizing the main innovation of the patent. Focus on what sets it apart and \"\n",
    "        \"why its significant in the field.\\n\"\n",
    "        \"2. **Summarize Core Claims**: Highlight the primary claims, explaining the unique aspects of the invention and any technical advantages.\\n\"\n",
    "        \"3. **Outline Potential Applications**: Describe potential uses and applications of the patent, tailoring language to ensure accessibility for non-technical readers.\\n\"\n",
    "        \"4. **Use Clear, Concise Language**: Avoid overly technical jargon unless absolutely necessary. Where technical terms are used, provide brief clarifications if they might be unfamiliar to general readers.\\n\"\n",
    "        \"5. **Maintain Accuracy and Neutral Tone**: Ensure that summaries remain factual and neutral, focusing on the patent content without adding subjective interpretations.\\n\"\n",
    "    ),\n",
    "    # llm=llm,\n",
    "    allow_delegation=False,\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    # max_iter=2  # Uncomment to limit iterations for efficiency\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "VSLzb9DC0-sC"
   },
   "outputs": [],
   "source": [
    "insight_agent = Agent(\n",
    "    role=\"Insight Strategist\",\n",
    "    goal=\"Generate innovative, unexpected insights by applying diverse mental models and angles to the problem\",\n",
    "    backstory=(\n",
    "        \"You are an extremely smart insight strategist with a knack for uncovering unique, out-of-the-box perspectives. \"\n",
    "        \"While you have expertise in the sector of {sector}, you are proficient in integrating knowledge and methodologies from various disciplines. \"\n",
    "        \"You excel at applying creative problem-solving techniques and mental models to generate fresh insights that challenge conventional thinking.\"\n",
    "    ),\n",
    "    instructions=(\n",
    "        \"1. **Analyze from Multiple Angles**: Start by viewing the problem from several distinct perspectives. Apply frameworks from different disciplines (e.g., psychology, economics, history, technology) to uncover novel insights.\\n\"\n",
    "        \"2. **Use Creative Thinking Techniques**: Leverage creativity tools like lateral thinking, mind mapping, or reverse engineering to challenge assumptions and identify unconventional connections.\\n\"\n",
    "        \"3. **Look for Unexpected Connections**: Identify patterns or relationships that might not be immediately obvious. Cross-pollinate ideas from unrelated fields to produce original perspectives.\\n\"\n",
    "        \"4. **Identify Potential Impacts**: For each insight, consider its potential implications. How could it reshape current thinking or lead to unexpected breakthroughs?\\n\"\n",
    "        \"5. **Contextualize with Sector Knowledge**: Use your sector expertise to ground your insights, but don't be afraid to stretch the boundaries and consider cross-industry trends and innovations.\\n\"\n",
    "        \"6. **Encourage Provocative Ideas**: Aim to generate insights that challenge the status quo, provoke thought, and inspire new ways of thinking.\\n\"\n",
    "        \"7. **Keep Insights Actionable**: While creativity is important, ensure that each insight is actionable, offering practical value or guiding strategic decisions.\\n\"\n",
    "    ),\n",
    "    # llm=llm,\n",
    "    allow_delegation=False,\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    # max_iter=2  # Uncomment to limit iterations for efficiency\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7GNV_YWXqO2"
   },
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "LIQ3AQMLXwhd"
   },
   "outputs": [],
   "source": [
    "generate_queries_task = Task(\n",
    "    description=(\n",
    "        \"Based on the provided challenge (e.g., 'biodegradable packaging for food'), generate a set of five related search keywords \"\n",
    "        \"that are concise, semantically related, and cover possible variations of the original query. \"\n",
    "        \"The queries should be brief and use as few words as possible to maximize relevance and clarity.\\n\\n\"\n",
    "\n",
    "        \"Context:\\n\"\n",
    "        \"Sector: {sector}\\n\"\n",
    "        \"Target Clients: {target_clients}\\n\"\n",
    "        \"Resources: {resources}\\n\"\n",
    "        \"Strategic Priorities: {strategic_priorities}\\n\"\n",
    "        \"Project Name: {project_name}\\n\"\n",
    "        \"Challenge Description: {challenge_description}\\n\"\n",
    "        \"Purpose: {purpose}\\n\"\n",
    "        \"Focus Constraints: {focus_constraints}\\n\\n\"\n",
    "\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Identify the key concepts in the challenge description.\\n\"\n",
    "        \"2. Generate search queries that reflect these key concepts while considering variations in phrasing, synonyms, or related terms.\\n\"\n",
    "        \"3. Ensure that the queries are concise and directly related to the core aspects of the challenge.\\n\"\n",
    "        \"4. Aim for a balance of broad and specific keywords that would lead to relevant information.\\n\"\n",
    "        \"5. Ensure that each query is distinct but related, to maximize the range of results without redundancy.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A list of five keywords related to the challenge: {challenge_description} that can be used for further searching. \"\n",
    "        \"Example: ['biodegradable food packaging', 'compostable packaging materials', 'eco-friendly packaging for food', 'plant-based packaging for perishables']\"\n",
    "    ),\n",
    "    tools=[],\n",
    "    agent=strategist_agent\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "nSezUTBac9_O"
   },
   "outputs": [],
   "source": [
    "patent_search_task = Task(\n",
    "    description=(\n",
    "        \"For each of the queries generated in Task 1, use the search tool to query Google Patents and retrieve \"\n",
    "        \"the top 5 most relevant search results for each query. Ensure that the patents selected are closely related to the original topic, \"\n",
    "        \"focusing on titles, abstracts, and URLs. Avoid extracting image links; only text-based information should be extracted. \"\n",
    "        \"Each query should be treated separately, ensuring the most relevant patents for each search term are identified.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A list of 25 total query results, combining the top 5 relevant patents for each of the 5 queries generated in Task 1. \"\n",
    "        \"Each result should include the title, URL, and abstract, formatted in a tabular structure. Do not include images.\"\n",
    "    ),\n",
    "    tools=[PatentSearchTool()],\n",
    "    agent=researcher_agent\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1-D1Qy1it2AN"
   },
   "outputs": [],
   "source": [
    "scholar_search_task = Task(\n",
    "    description=(\n",
    "        \"For each of the queries generated in Task 1, use the search tool to query Google Scholar and retrieve \"\n",
    "        \"the top 5 most relevant search results for each query. Ensure that the articles selected are closely related to the original topic, \"\n",
    "        \"focusing on titles, abstracts, and URLs. Avoid extracting image links or full-text articles; only text-based information should be extracted. \"\n",
    "        \"Each query should be treated separately to identify the most relevant academic articles for each search term.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A list of 25 total query results, combining the top 5 relevant academic articles for each of the 5 queries generated in Task 1. \"\n",
    "        \"Each result should include the title, URL, and abstract, presented in a tabular format. Do not include images or full-text links.\"\n",
    "    ),\n",
    "    tools=[ScholarSearchTool()],\n",
    "    agent=researcher_agent\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "sLY72c8adyBO"
   },
   "outputs": [],
   "source": [
    "scrape_content_task = Task(\n",
    "    description=(\n",
    "        \"For each of the top 5 patent results obtained in Task 2, use the scrape tool to capture the detailed content from the patent pages. \"\n",
    "        \"Ensure that the abstract, claims, and any relevant technical information are accurately captured for each patent. \"\n",
    "        \"Only extract text-based contentignore images, diagrams, or any non-textual elements. \"\n",
    "        \"The focus should be on key textual information that provides insights into the patents claims, technical details, and innovations.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A collection of the scraped content for each patent, presented in a structured format. \"\n",
    "        \"Each patent should include the abstract, claims, and any important technical details, with the text organized for easy analysis.\"\n",
    "    ),\n",
    "    tools=[scrape_tool],\n",
    "    agent=scraper_agent\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "KNBBjKaJNzwn"
   },
   "outputs": [],
   "source": [
    "select_patents_task = Task(\n",
    "    description=(\n",
    "        \"Review all the patents identified in the previous task. For each patent, assess its relevance based on the \"\n",
    "        \"provided context, including the challenge description, sector, target clients, and strategic priorities. \"\n",
    "        \"Use your judgment to determine how closely the patent aligns with the core objectives of the project.\\n\\n\"\n",
    "\n",
    "        \"Context:\\n\"\n",
    "        \"Sector: {sector}\\n\"\n",
    "        \"Target Clients: {target_clients}\\n\"\n",
    "        \"Resources: {resources}\\n\"\n",
    "        \"Strategic Priorities: {strategic_priorities}\\n\"\n",
    "        \"Project Name: {project_name}\\n\"\n",
    "        \"Challenge Description: {challenge_description}\\n\"\n",
    "        \"Purpose: {purpose}\\n\"\n",
    "        \"Focus Constraints: {focus_constraints}\\n\\n\"\n",
    "\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Review the patents title, abstract, claims, and technical content.\\n\"\n",
    "        \"2. Evaluate how well the patent addresses the key challenges described in the original topic and purpose.\\n\"\n",
    "        \"3. Rank the patents in order of relevance based on how well they align with the projects objectives.\\n\"\n",
    "        \"4. Provide a brief explanation of why each patent was selected, focusing on its relevance to the sector, the challenge, and strategic priorities.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A shortlist of the 10 most relevant patents to the original challenge: {challenge_description}. \"\n",
    "        \"Present a table with the following columns: patent number, title, awardees, URL, a summary of the abstract, and a brief rationale explaining why each patent is relevant.\"\n",
    "    ),\n",
    "    agent=strategist_agent\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "0VnvOMMxuVvA"
   },
   "outputs": [],
   "source": [
    "select_papers_task = Task(\n",
    "    description=(\n",
    "        \"Review all the research papers identified in the previous task. For each paper, assess its relevance based on the \"\n",
    "        \"provided context, including the challenge description, sector, target clients, and strategic priorities. \"\n",
    "        \"Use your judgment to determine how closely the paper aligns with the core objectives of the project.\\n\\n\"\n",
    "\n",
    "        \"Context:\\n\"\n",
    "        \"Sector: {sector}\\n\"\n",
    "        \"Target Clients: {target_clients}\\n\"\n",
    "        \"Resources: {resources}\\n\"\n",
    "        \"Strategic Priorities: {strategic_priorities}\\n\"\n",
    "        \"Project Name: {project_name}\\n\"\n",
    "        \"Challenge Description: {challenge_description}\\n\"\n",
    "        \"Purpose: {purpose}\\n\"\n",
    "        \"Focus Constraints: {focus_constraints}\\n\\n\"\n",
    "\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Review the papers title, abstract, and key findings.\\n\"\n",
    "        \"2. Evaluate how well the paper addresses the key challenges outlined in the original topic and purpose.\\n\"\n",
    "        \"3. Rank the papers in order of relevance based on how well they align with the projects objectives.\\n\"\n",
    "        \"4. Provide a brief explanation of why each paper was selected, focusing on its relevance to the sector, the challenge, and strategic priorities.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A shortlist of the 10 most relevant papers to the original challenge: {challenge_description}. \"\n",
    "        \"Present a table with the following columns: paper title, authors, URL, summary of the abstract, and a brief rationale explaining why each paper is relevant.\"\n",
    "    ),\n",
    "    agent=strategist_agent\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Ln9seVY6riHC"
   },
   "outputs": [],
   "source": [
    "summarize_task = Task(\n",
    "    description=(\n",
    "        \"Based on the selected content from the previous step, first reproduce the output table from the previous task as it is, \"\n",
    "        \"with the details of the patents and papers.\\n\\n\"\n",
    "\n",
    "        \"Next, write a concise, clear, and comprehensive summary that explains the content of the selected patents and research papers. \"\n",
    "        \"This summary should focus on how these findings relate to the original challenge and purpose. Ensure the summary includes the following:\\n\"\n",
    "        \"1. **Overview of Innovations**: Highlight the key technological innovations found in the papers and patents.\\n\"\n",
    "        \"2. **Research Centers and Trends**: Mention where major research is being conducted, which institutions or industries are leading in this space, and any significant geographical concentrations.\\n\"\n",
    "        \"3. **Research Age and Relevance**: Discuss how recent the research is and whether it's still relevant to the current challenge.\\n\"\n",
    "        \"4. **Technical Details Translation**: Translate technical content into digestible information for a non-technical audience, ensuring clarity and accessibility.\\n\"\n",
    "        \"5. **Relevance to Challenge**: Tie each piece of research to the challenge, explaining how it might contribute to solving the problem at hand.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"Reproduce the output table from the previous task, followed by a clear and concise summary of the patents and papers. \"\n",
    "        \"The summary should cover innovations, research trends, the age of research, and its relevance to the challenge.\"\n",
    "    ),\n",
    "    tools=[],  # Placeholder tool for summarizing content\n",
    "    agent=writer_agent\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "A67EDQIh2d9K"
   },
   "outputs": [],
   "source": [
    "insight_task = Task(\n",
    "    description=(\n",
    "        \"Generate five novel and thought-provoking insights based on the previous research findings, including the patents and research papers. \"\n",
    "        \"An insight should uncover counterintuitive or surprising aspects about the current state of things, often by identifying underlying tensions \"\n",
    "        \"or contradictions in the data. For example, there could be a disconnect between what consumers say they want and what is actually available, \"\n",
    "        \"or conflicting trends that reveal hidden opportunities. Ensure that each insight is original and provides fresh perspectives.\\n\\n\"\n",
    "\n",
    "        \"Example Insights:\\n\"\n",
    "        \"- There is a disconnect between the expert recommendation of intuitive eating and people's ingrained habits of following external food rules, \"\n",
    "        \"relying on data rather than bodily cues.\\n\"\n",
    "        \"- There's a mismatch between peoples desire for natural, whole-food nutrition and the prevalence of artificial supplements in the performance nutrition market.\\n\\n\"\n",
    "\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. **Identify Tensions or Contradictions**: Look for contradictions or tensions in the research that might lead to surprising insights.\\n\"\n",
    "        \"2. **Generate Catchy Titles**: Each insight should have a compelling and catchy title that summarizes the tension or novel idea.\\n\"\n",
    "        \"3. **Provide Context**: After each insight, write a brief description that explains the underlying tension and the implications of the finding.\\n\"\n",
    "        \"4. **Cite Relevant Sources**: Reference the specific papers or patents from previous tasks that support or relate to each insight.\\n\"\n",
    "        \"5. **Explain Relevance to the Company**: For each insight, explain how it could impact the company and the challenge at hand, and why it is important for their strategic priorities.\\n\"\n",
    "        \"6. **Link to Challenge and Strategic Priorities**: Ensure that each insight is directly tied to the challenge, sector, target clients, and strategic priorities of the company.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A set of five novel, insightful, and cleverly formulated insights that address key contradictions or surprising trends in the research. \"\n",
    "        \"Each insight should include a catchy title, a brief description, relevant sources from the research, and an explanation of how it applies to the specific company and its strategic priorities.\"\n",
    "    ),\n",
    "    tools=[],  # Placeholder tool for summarizing content\n",
    "    context=[summarize_task],  # Link to the summary task for prior research context\n",
    "    agent=insight_agent\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "b9MWEQ8furNp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "FNxv8aD8BqZr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEsGAgcoZVNp"
   },
   "source": [
    "\n",
    "# Creating crews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "_l71CkXYKosD"
   },
   "outputs": [],
   "source": [
    "#Patents crew\n",
    "\n",
    "patents_crew = Crew(\n",
    "    agents=[\n",
    "        strategist_agent,\n",
    "        researcher_agent,\n",
    "        #scraper_agent,\n",
    "        writer_agent,\n",
    "\n",
    "      ],\n",
    "\n",
    "    tasks=[\n",
    "        generate_queries_task,\n",
    "        patent_search_task,\n",
    "        #patent_search_task,\n",
    "        #scrape_content_task,\n",
    "        select_patents_task,\n",
    "        summarize_task,\n",
    "\n",
    "    ],\n",
    "\n",
    "    #process=Process.hierarchical,\n",
    "    #manager_agent=manager,\n",
    "    #manager_llm=manager_llm,\n",
    "\n",
    "    process=Process.sequential,\n",
    "    #planning=True,\n",
    "    verbose=True,\n",
    "\t  memory=False,\n",
    "    #cache=False,\n",
    "    #share_crew=False,\n",
    "    #output_log_file=\"outputs/content_plan_log.txt\",\n",
    "    #max_rpm=50,\n",
    "    #output_name='patents_output'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JA20KTBHuq5",
    "outputId": "371180bf-9fde-46b2-c35c-595520845c10"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "#Scholar crew\n",
    "\n",
    "scholar_crew = Crew(\n",
    "    agents=[\n",
    "        strategist_agent,\n",
    "        researcher_agent,\n",
    "        #scraper_agent,\n",
    "        writer_agent,\n",
    "      ],\n",
    "\n",
    "    tasks=[\n",
    "        generate_queries_task,\n",
    "        scholar_search_task,\n",
    "        #patent_search_task,\n",
    "        #scrape_content_task,\n",
    "        select_papers_task,\n",
    "        summarize_task,\n",
    "\n",
    "    ],\n",
    "\n",
    "    #process=Process.hierarchical,\n",
    "    #manager_agent=manager,\n",
    "    #manager_llm=manager_llm,\n",
    "\n",
    "    process=Process.sequential,\n",
    "    #planning=True,\n",
    "    verbose=True,\n",
    "\t  memory=False,\n",
    "    #cache=False,\n",
    "    #share_crew=False,\n",
    "    #output_log_file=\"outputs/content_plan_log.txt\",\n",
    "    #max_rpm=50,\n",
    "    output_name='scholar_output'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRI4bqKf65tY",
    "outputId": "55556662-389c-4abe-bae9-793e26df7831"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "#Insights crew\n",
    "\n",
    "insights_crew = Crew(\n",
    "    agents=[\n",
    "        insight_agent,\n",
    "      ],\n",
    "\n",
    "    tasks=[\n",
    "        insight_task,\n",
    "\n",
    "    ],\n",
    "\n",
    "    #process=Process.hierarchical,\n",
    "    #manager_agent=manager,\n",
    "    #manager_llm=manager_llm,\n",
    "\n",
    "    process=Process.sequential,\n",
    "    #planning=True,\n",
    "    verbose=True,\n",
    "\t  memory=False,\n",
    "    #cache=False,\n",
    "    #share_crew=False,\n",
    "    #output_log_file=\"outputs/content_plan_log.txt\",\n",
    "    #max_rpm=50,\n",
    "    output_name='insights'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJSI3CkPvoca"
   },
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "-8Dl2UMcINgQ"
   },
   "outputs": [],
   "source": [
    "input_1 = {\n",
    "    # Company inputs\n",
    "    \"company_name\": \"Coffee Bean Technologies\",\n",
    "    \"sector\": \"Coffee Manufacturing\",\n",
    "    \"target_clients\": \"HoReCa and brands who sell coffee\",\n",
    "    \"resources\": \"Coffee plantations, machinery for harvesting, toasting, brewing, packaging; distribution network\",\n",
    "    \"strategic_priorities\": \"Identifying new products to penetrate existing and new markets, particularly companies targeting younger consumers\",\n",
    "\n",
    "    # Project inputs\n",
    "    \"project_name\": \"Coffee Bean Coating Technologies\",\n",
    "    \"challenge_description\": \"Identifying technologies that enable coating coffee beans with a thin layer containing color and nutrients\",\n",
    "    \"purpose\": \"Enriching cofee beans by coating them with colors and nutrients \",\n",
    "    \"focus_constraints\": \" \",\n",
    "\n",
    "}\n",
    "\n",
    "input_2 = {\n",
    "    # Company inputs\n",
    "    \"company_name\": \"Flipper Eyewear\",\n",
    "    \"sector\": \"Consumer Goods / Eyewear\",\n",
    "    \"target_clients\": \"Individuals seeking versatile and sustainable eyewear solutions\",\n",
    "    \"resources\": \"Innovative eyewear designs with interchangeable lenses and clip-on accessories\",\n",
    "    \"strategic_priorities\": \"Enhance product versatility and user convenience; expand market presence; leverage social media\",\n",
    "\n",
    "    # Project inputs\n",
    "    \"project_name\": \"Development of Eco-Friendly Materials for Eyewear\",\n",
    "    \"challenge_description\": \"Develop eco-friendly materials for frames and lenses to meet growing consumer demand for sustainable products\",\n",
    "    \"purpose\": \"Align product offerings with environmental sustainability trends to attract eco-conscious consumers and differentiate the brand in the competitive eyewear market\",\n",
    "    \"focus_constraints\": \"Ensure materials meet durability standards while maintaining affordability\",\n",
    "}\n",
    "\n",
    "\n",
    "input_3 = {\n",
    "    # Company inputs\n",
    "    \"company_name\": \"Fastrock Coffee\",\n",
    "    \"sector\": \"Food & Beverage / Coffee\",\n",
    "    \"target_clients\": \"Retailers, foodservice providers, convenience stores, and hospitality industries\",\n",
    "    \"resources\": \"Vertically integrated supply chain; advanced roasting and manufacturing facilities; expertise in sustainable sourcing\",\n",
    "    \"strategic_priorities\": \"Expand global sourcing and supply chain capabilities; invest in sustainable and traceable supply chain practices; enhance product innovation\",\n",
    "\n",
    "    # Project inputs\n",
    "    \"project_name\": \"Utilization of Coffee By-Products for New Revenue Streams\",\n",
    "    \"challenge_description\": \"Explore alternative uses for coffee by-products to create new revenue streams and reduce waste\",\n",
    "    \"purpose\": \"Develop processes that are cost-effective and environmentally sustainable\",\n",
    "    \"focus_constraints\": \"Ensure new processes are economically viable and align with the company's sustainabilitygoals\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqCtS6TV62GQ"
   },
   "source": [
    "# Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "rOUZiO5-rzQV"
   },
   "outputs": [],
   "source": [
    "from crewai.flow.flow import Flow, start, listen\n",
    "from crewai import Flow\n",
    "from crewai.flow.flow import listen,start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "5v9TGSWZ61cd"
   },
   "outputs": [],
   "source": [
    "# class InsightGen(Flow):\n",
    "#     @start()\n",
    "#     def find_patents(self):\n",
    "#         patents_output = patents_crew.kickoff(inputs=inputs)\n",
    "#         self.state[\"patents_crew_results\"] = patents_output\n",
    "#         return patents_output\n",
    "\n",
    "#     @start()\n",
    "#     def find_scholar(self):\n",
    "#         scholar_output = scholar_crew.kickoff(inputs=inputs)\n",
    "#         self.state[\"scholar_crew_results\"] = scholar_output\n",
    "#         return scholar_output\n",
    "\n",
    "#     @listen(find_patents)\n",
    "#     @listen(find_scholar)\n",
    "#     def generate_insights(self):\n",
    "#         patents_output = self.state[\"patents_crew_results\"]\n",
    "#         scholar_output = self.state[\"scholar_crew_results\"]\n",
    "\n",
    "#         # Combine the outputs into a single dictionary\n",
    "#         combined_output = {\n",
    "#             \"patents\": patents_output,\n",
    "#             \"scholar\": scholar_output\n",
    "#         }\n",
    "\n",
    "#         # Pass the combined output as a single argument\n",
    "#         insights = insights_crew.kickoff(combined_output)\n",
    "#         self.state[\"insights_crews_results\"] = insights\n",
    "#         return insights\n",
    "\n",
    "\n",
    "# class InsightGen(Flow):\n",
    "#     @start()\n",
    "#     def find_patents(self):\n",
    "#         patents_output = patents_crew.kickoff(inputs=inputs)\n",
    "#         self.state[\"patents_crew_results\"] = patents_output\n",
    "#         return patents_output\n",
    "\n",
    "#     @start()\n",
    "#     def find_scholar(self):\n",
    "#         scholar_output = scholar_crew.kickoff(inputs=inputs)\n",
    "#         self.state[\"scholar_crew_results\"] = scholar_output\n",
    "#         return scholar_output\n",
    "\n",
    "#     @listen(find_patents)\n",
    "#     @listen(find_scholar)\n",
    "#     def generate_insights(self):\n",
    "#         patents_output = self.state[\"patents_crew_results\"]\n",
    "#         scholar_output = self.state[\"scholar_crew_results\"]\n",
    "\n",
    "#         # Combine the outputs into a single dictionary\n",
    "#         combined_output = {\n",
    "#             \"patents\": patents_output,\n",
    "#             \"scholar\": scholar_output,\n",
    "#             \"sector\": inputs[\"sector\"], # Added sector to combined_output\n",
    "#             \"target_clients\": inputs[\"target_clients\"],\n",
    "#             \"resources\": inputs[\"resources\"],\n",
    "#             \"strategic_priorities\": inputs[\"strategic_priorities\"],\n",
    "#             \"project_name\": inputs[\"project_name\"],\n",
    "#             \"challenge_description\": inputs[\"challenge_description\"],\n",
    "#             \"purpose\": inputs[\"purpose\"],\n",
    "#             \"focus_constraints\": inputs[\"focus_constraints\"]\n",
    "#         }\n",
    "\n",
    "#         # Pass the combined output as a single argument\n",
    "#         insights = insights_crew.kickoff(combined_output)\n",
    "#         self.state[\"insights_crews_results\"] = insights\n",
    "#         return insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "QeTRcwnRTLX5"
   },
   "outputs": [],
   "source": [
    "class InsightGen(Flow):\n",
    "    @start()\n",
    "    async def start_parallel_execution(self):\n",
    "        patents_future = patents_crew.kickoff_async(inputs=input_3)\n",
    "        scholar_future = scholar_crew.kickoff_async(inputs=input_3)\n",
    "\n",
    "        patents_output, scholar_output = await asyncio.gather(patents_future, scholar_future)\n",
    "\n",
    "        # Extract raw output from CrewOutput objects\n",
    "        patents_data = patents_output.raw_output if hasattr(patents_output, 'raw_output') else str(patents_output)\n",
    "        scholar_data = scholar_output.raw_output if hasattr(scholar_output, 'raw_output') else str(scholar_output)\n",
    "\n",
    "        self.state[\"patents_crew_results\"] = patents_data\n",
    "        self.state[\"scholar_crew_results\"] = scholar_data\n",
    "        return {\"patents\": patents_data, \"scholar\": scholar_data}\n",
    "\n",
    "    @listen(start_parallel_execution)\n",
    "    def generate_insights(self, parallel_results):\n",
    "        combined_output = {\n",
    "            **parallel_results,\n",
    "            \"sector\": input_3[\"sector\"],\n",
    "            \"target_clients\": input_3[\"target_clients\"],\n",
    "            \"resources\": input_3[\"resources\"],\n",
    "            \"strategic_priorities\": input_3[\"strategic_priorities\"],\n",
    "            \"project_name\": input_3[\"project_name\"],\n",
    "            \"challenge_description\": input_3[\"challenge_description\"],\n",
    "            \"purpose\": input_3[\"purpose\"],\n",
    "            \"focus_constraints\": input_3[\"focus_constraints\"]\n",
    "        }\n",
    "\n",
    "        insights = insights_crew.kickoff(combined_output)\n",
    "\n",
    "        # Extract raw output from CrewOutput\n",
    "        insights_data = insights.raw_output if hasattr(insights, 'raw_output') else str(insights)\n",
    "\n",
    "        self.state[\"insights_crews_results\"] = insights_data\n",
    "        return insights_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "rseWP6eouIqR"
   },
   "outputs": [],
   "source": [
    "flow = InsightGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "eO5D5Az53i71",
    "outputId": "86672ba7-06b3-4b42-b896-ed50d7f55c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved as crewai_flow.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"150%\"\n",
       "            height=\"600\"\n",
       "            src=\"./crewai_flow.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7e46864fd2a0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.plot()\n",
    "\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='./crewai_flow.html', width='150%', height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Mq7mNN9Koex"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_A9vfNI2ols"
   },
   "source": [
    "# Run Crews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "x9cIVZRTQh35"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_flow():\n",
    "  \"\"\"Helper function to run the flow using the existing loop.\"\"\"\n",
    "  return await flow.kickoff_async()\n",
    "\n",
    "# Get the current event loop or create a new one if it doesn't exist.\n",
    "loop = asyncio.get_event_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "KAG57F_MQs9T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TDZ_M8bLMUg",
    "outputId": "30cf1e14-02f7-4bb0-f836-bc1caa7bd9ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mStrategist\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the provided challenge (e.g., 'biodegradable packaging for food'), generate a set of five related search keywords that are concise, semantically related, and cover possible variations of the original query. The queries should be brief and use as few words as possible to maximize relevance and clarity.\n",
      "\n",
      "Context:\n",
      "Sector: Food & Beverage / Coffee\n",
      "Target Clients: Retailers, foodservice providers, convenience stores, and hospitality industries\n",
      "Resources: Vertically integrated supply chain; advanced roasting and manufacturing facilities; expertise in sustainable sourcing\n",
      "Strategic Priorities: Expand global sourcing and supply chain capabilities; invest in sustainable and traceable supply chain practices; enhance product innovation\n",
      "Project Name: Utilization of Coffee By-Products for New Revenue Streams\n",
      "Challenge Description: Explore alternative uses for coffee by-products to create new revenue streams and reduce waste\n",
      "Purpose: Develop processes that are cost-effective and environmentally sustainable\n",
      "Focus Constraints: Ensure new processes are economically viable and align with the company's sustainabilitygoals\n",
      "\n",
      "Instructions:\n",
      "1. Identify the key concepts in the challenge description.\n",
      "2. Generate search queries that reflect these key concepts while considering variations in phrasing, synonyms, or related terms.\n",
      "3. Ensure that the queries are concise and directly related to the core aspects of the challenge.\n",
      "4. Aim for a balance of broad and specific keywords that would lead to relevant information.\n",
      "5. Ensure that each query is distinct but related, to maximize the range of results without redundancy.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mStrategist\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "1. 'coffee by-product utilization'  \n",
      "2. 'sustainable coffee waste repurposing'  \n",
      "3. 'revenue streams from coffee residues'  \n",
      "4. 'environmental benefits of coffee by-products'  \n",
      "5. 'innovative uses for coffee waste'\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mFor each of the queries generated in Task 1, use the search tool to query Google Patents and retrieve the top 5 most relevant search results for each query. Ensure that the patents selected are closely related to the original topic, focusing on titles, abstracts, and URLs. Avoid extracting image links; only text-based information should be extracted. Each query should be treated separately, ensuring the most relevant patents for each search term are identified.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mScholar search tool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": \\\"coffee by-product utilization\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "{\n",
      "  \"searchParameters\": {\n",
      "    \"q\": \"coffee by-product utilization\",\n",
      "    \"type\": \"scholar\",\n",
      "    \"num\": 5,\n",
      "    \"autocorrect\": false,\n",
      "    \"tbs\": \"qdr:d\",\n",
      "    \"engine\": \"google-scholar\"\n",
      "  },\n",
      "  \"organic\": [\n",
      "    {\n",
      "      \"title\": \"Utilization of coffee by-products as profitable foods-a mini review\",\n",
      "      \"link\": \"https://iopscience.iop.org/article/10.1088/1755-1315/672/1/012077/meta\",\n",
      "      \"publicationInfo\": \"M Muzaifa, F Rahmi - IOP Conference Series: Earth and \\u2026, 2021 - iopscience.iop.org\",\n",
      "      \"snippet\": \"\\u2026 The coffee pulp is the largest byproduct acquired during wet coffee processing, so the primary attention has given to the byproduct. The by-products have been studied from the \\u2026\",\n",
      "      \"year\": 2021,\n",
      "      \"citedBy\": 28\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"A nutritional analysis on the by-product coffee husk and its potential utilization in food production\",\n",
      "      \"link\": \"http://stud.epsilon.slu.se/8486/\",\n",
      "      \"publicationInfo\": \"E Bondesson - 2015 - stud.epsilon.slu.se\",\n",
      "      \"snippet\": \"\\u2026 on the chemical composition in the by-product coffee husk and examine its potential in food \\u2026 focus on utilization outside food production are also excluded. In the study, coffee husk was \\u2026\",\n",
      "      \"year\": 2015,\n",
      "      \"citedBy\": 57\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Coffee By-Products: Economic Opportunities for Sustainability and Innovation in the Coffee Industry\",\n",
      "      \"link\": \"https://www.mdpi.com/2504-3900/89/1/6\",\n",
      "      \"publicationInfo\": \"M Peluso - Proceedings, 2023 - mdpi.com\",\n",
      "      \"snippet\": \"\\u2026 benefits stemming from the utilization of coffee by-products, and \\u2026 The economic value attributed to the coffee by-product \\u2026 the utilization, processing, and commercialization of coffee by-\\u2026\",\n",
      "      \"year\": 2023,\n",
      "      \"citedBy\": 5\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"The wastes of coffee bean processing for utilization in food: a review\",\n",
      "      \"link\": \"https://link.springer.com/article/10.1007/s13197-021-05032-5\",\n",
      "      \"publicationInfo\": \"SS Arya, R Venkatram, PR More, P Vijayan - Journal of Food Science and \\u2026, 2022 - Springer\",\n",
      "      \"snippet\": \"\\u2026 coffee flour further dehulling of bean to produce coffee husk. The coffee silverskin (CS) is an integument of coffee \\u2026 as a main by-product of the roasting process and spent coffee ground (\\u2026\",\n",
      "      \"year\": 2022,\n",
      "      \"citedBy\": 117\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Applications of compounds from coffee processing by-products\",\n",
      "      \"link\": \"https://www.mdpi.com/2218-273X/10/9/1219\",\n",
      "      \"publicationInfo\": \"A Iriondo-DeHond, M Iriondo-DeHond, MD Del Castillo - Biomolecules, 2020 - mdpi.com\",\n",
      "      \"snippet\": \"\\u2026 coffee beverage is prepared at home or processed for soluble/instant coffee leading to the generation of the last coffee by-product, \\u2026 for this by-product and the diversification of the coffee \\u2026\",\n",
      "      \"year\": 2020,\n",
      "      \"citedBy\": 108\n",
      "    }\n",
      "  ],\n",
      "  \"credits\": 1\n",
      "}\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "my_insights = await flow.kickoff_async() #should be await flow.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "6yP4nZJmC41N"
   },
   "outputs": [],
   "source": [
    "# Run crew\n",
    "\n",
    "# summaries= crew.kickoff(inputs=inputs)\n",
    "\n",
    "# Save the report to a file\n",
    "#write_to_file(summaries, directory='/content/drive/MyDrive/Creative_AI/research_engine/Outputs', filename='playground.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "eiGp0EqGqyoF"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "id": "8GSzE56Jbkix",
    "outputId": "d4488367-3d9c-4c5f-d89b-92495b6bbfb3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'### Insight 1: \"The Hidden Value of Coffee Pulp: From Tossed Trash to Culinary Treasure\"\\n**Context**: Despite its abundant nutritional and economic potential, coffee pulp is often discarded, highlighting a contradiction between environmental awareness and practical application. Research indicates that while consumers are increasingly drawn to healthful, whole-food ingredients, coffee pulprich in antioxidantsremains underutilized in the food industry. \\n**Source**: Utilization of coffee by-products as profitable foods - a mini review. [Link](https://iopscience.iop.org/article/10.1088/1755-1315/672/1/012077/meta)\\n**Relevance**: This insight can guide the company in innovating product lines that incorporate coffee pulp as a delicious, nutritious ingredient, aligning with consumer preferences for natural foods and enhancing brand sustainability.\\n\\n### Insight 2: \"Eco-Friendly Fuels: Coffee Grounds as Renewable Energy Sources\"\\n**Context**: Although the market increasingly calls for sustainable energy solutions, the concept of recycling coffee waste into fuels contradicts consumers\\' perceptions of \\'natural\\' versus recyclable. The findings suggest that spent coffee grounds could be more valuable as energy sources than they are when composted, yet consumers remain skeptical about processed fuels.\\n**Source**: Comprehensive evaluation of liquid and solid fuels derived from recycled coffee waste. [Link](https://www.sciencedirect.com/science/article/pii/S0921344919303416)\\n**Relevance**: Embracing this duality may allow the company to pioneer biofuels using coffee waste, appealing to eco-conscious consumers and positioning the brand at the forefront of sustainable innovation.\\n\\n### Insight 3: \"Building Blocks of Sustainability: Coffee Waste in Construction\"\\n**Context**: A paradox exists between the dire need for sustainable construction materials and the unexplored potential of coffee grounds in this domain. While there is a growing trend to address sustainability in architecture, many firms continue to overlook innovative materials sourced from waste, such as coffee.\\n**Source**: Recycling of spent coffee grounds in construction materials: A review. [Link](https://www.sciencedirect.com/science/article/pii/S0959652621000573)\\n**Relevance**: This insight can lead to cross-industry collaborations, enabling the company to diversify revenue streams by participating in green building projects, thus reinforcing its commitment to sustainability.\\n\\n### Insight 4: \"The Nutritional Paradox: Rising Demand vs. Artificial Ingredients\"\\n**Context**: Consumers express a desire for natural and organic ingredients rather than artificial supplements in the health sector. However, the coffee industry still heavily relies on processed coffee derivatives, which often include additives. This contradiction reveals a significant opportunity for innovation in creating wholesome products from coffee by-products.\\n**Source**: Value-added products from coffee waste: a review. [Link](https://www.mdpi.com/1420-3049/28/8/3562)\\n**Relevance**: By leveraging this trend, the company can develop clean-label products that incorporate coffee waste, thereby capturing a growing health-conscious market and reinforcing its brand integrity.\\n\\n### Insight 5: \"From Waste to Wonder: The Circular Economy of Coffee\"\\n**Context**: While the concept of the circular economy is gaining traction, the coffee industry\\'s transformation is hindered by a linear mindset that minimizes waste management. Research indicates that adopting a biorefinery approach to coffee waste fosters a sustainable model; yet the industry remains slow to adapt.\\n**Source**: Biorefinery of spent coffee grounds waste: A pathway toward the circular bioeconomy. [Link](https://www.sciencedirect.com/science/article/pii/S0960852420300900)\\n**Relevance**: This insight challenges the company to lead the charge towards a circular economy model by developing products that process coffee waste into multiple applications, thus emphasizing sustainability and innovation at every stage of production.\\n\\nIn conclusion, these insights not only highlight contradictions and unexpected opportunities within the coffee by-product landscape but also provide actionable strategies that align with the company\\'s strategic priorities for innovation, sustainability, and profitability.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "id": "o7dUGtjvwc7k"
   },
   "outputs": [],
   "source": [
    "print(my_insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "57BWgROnwl0V"
   },
   "outputs": [],
   "source": [
    "print(type(my_insights))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
